{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409501d-53c7-4e76-86b6-5f7a430dc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1-clusterScore\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60810f7a-f47f-4d03-a039-decf79e235bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可跑版 使用angle-bert-base-uncased-nli-en-v1进行single-pass  ByTitle\n",
    "# update:加了评估，可对第一天结果进行评估与记录评估结果\n",
    "# updata:ByTitle\n",
    "# update：使用angle加载\n",
    "# updata:to .py :single-pass-ByTitle-angle-bert-AngleLOAD-Eval.py\n",
    "# update:保存predict_clusters\n",
    "# update:3.6 load FIX data\n",
    "# 暂时聚类最佳:angle-bert ANGLE_LOAD thresold = 0.975\n",
    "# update：3.9 新增Score计算（以angle-bert ANGLE_LOAD thresold = 0.975为例）\n",
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from angle_emb import AnglE\n",
    "\n",
    "\n",
    "# yes! 聚类评估！！！可跑 TP, FP, TN, FN 得到RI、Precision、Recall、F1，ARI\n",
    "# update:单个成簇的处理\n",
    "from itertools import combinations\n",
    "from math import comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e99549-bbc8-4223-b0be-54e28bc5024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用angle加载\n",
    "model_id = '../NewsAthmTask2/models/angle-bert-base-uncased-nli-en-v1'\n",
    "angle = AnglE.from_pretrained(model_id, pooling_strategy='cls_avg').cuda()\n",
    "\n",
    "# 加载数据\n",
    "# data = pd.read_csv('Data231202-231211.csv')\n",
    "# data = pd.read_csv('./Data231202-231211_FIX/Data231202_newDATA.csv')\n",
    "data = pd.read_csv('./Data231202-231211/Data231202.csv')\n",
    "\n",
    "\n",
    "# 将日期转换为日期时间格式\n",
    "data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "# 获取唯一日期列表\n",
    "dates = data['pub_time'].dt.date.unique()\n",
    "\n",
    "# 定义聚类中心更新函数\n",
    "def update_cluster_center(cluster):\n",
    "    # cluster_embeddings = sbert_model.encode(cluster)\n",
    "    cluster_embeddings = angle.encode(cluster, to_numpy=True) # 使用angle加载\n",
    "     \n",
    "    return np.mean(cluster_embeddings, axis=0)\n",
    "# 设置阈值\n",
    "threshold = 0.972\n",
    "\n",
    "# 对于每个日期\n",
    "cluster_results = []\n",
    "cnt = 0\n",
    "for date in dates:\n",
    "    print(cnt)\n",
    "    cnt+=1\n",
    "    # 获取该日期的新闻标题\n",
    "    news_data = data[data['pub_time'].dt.date == date]['title'].tolist()\n",
    "    # 获取该日期的新闻正文\n",
    "    # news_data = data[data['pub_time'].dt.date == date]['body'].tolist() # ByBody\n",
    "\n",
    "    # 使用SBERT模型获取语义向量\n",
    "    # embeddings = sbert_model.encode(news_data)\n",
    "\n",
    "    embeddings = angle.encode(news_data, to_numpy=True) # 使用angle加载\n",
    "\n",
    "    # 定义当天的簇列表\n",
    "    daily_clusters = []\n",
    "\n",
    "    # 对于每个新闻数据\n",
    "    # for i, embedding in enumerate(data_vec):\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        # 如果簇列表为空，则新开一个簇\n",
    "        if not daily_clusters:\n",
    "            # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "            daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "            continue\n",
    "\n",
    "        # 计算当前数据点与各个簇中心的相似度\n",
    "        # similarities = [cosine_similarity([embedding], [cluster['center']])[0][0] for cluster in daily_clusters]\n",
    "        similarities = [cosine_similarity([embedding], [cluster['center']])[0][0] for cluster in daily_clusters]\n",
    "\n",
    "        # 找到最大相似度及其对应的簇索引\n",
    "        max_similarity = max(similarities)\n",
    "        max_index = similarities.index(max_similarity)\n",
    "\n",
    "        # 如果最大相似度大于阈值，则将当前数据点加入对应簇，并更新簇中心\n",
    "        if max_similarity > threshold:\n",
    "            # daily_clusters[max_index]['members'].append(news_data[i])\n",
    "            daily_clusters[max_index]['members'].append(i) # 改为存index\n",
    "            daily_clusters[max_index]['news'].append(news_data[i]) # 改为存index\n",
    "            # daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['members'])\n",
    "            # daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['members'],news_data)\n",
    "            daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['news'])\n",
    "        # 否则新开一个簇\n",
    "        else:\n",
    "            # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "            daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "\n",
    "    # 将当天的簇信息添加到结果列表中\n",
    "    cluster_results.append({'date': date, 'clusters': daily_clusters})\n",
    "\n",
    "# 评估\n",
    "#    true_clusters = [[0],[1],[2,16],[3],[4,6,22,50,73,87],[5],[7],[8,61],[9],[10,77],[11],[12],[13],\n",
    "# [14,29,41,51,59,67,78,84],[15],[17],[18],[19],[20],[21,68],[23],[24],[25],[26],\n",
    "# [27],[28],[30],[31],[32],[33],[34],[35,55],[36],[37],[38],[39],[40],[42],[43,64],\n",
    "# [44],[45],[46],[47,53,88],[48],[49],[52],[54],[56],[57],[58],[60],[62],[63],[65],\n",
    "# [66],[69],[70],[71],[72],[74],[75],[76],[79],[80],[81],[82],[83],[85],[86],\n",
    "# [89],[90],[91],[92],[93],[94],[95]]\n",
    "\n",
    "# update:3.6 新true_clusters for FIX\n",
    "# true_clusters = [[0],[1,4,6,23,28,41],[2],[3],[5],[7],[8],[9,31],[10],[11],[12],[13],[14],[15],[16],[18],[19],[20],[17,21,38],[22],[24],[25],[26],[27],[29],[30],[32],[33],[34],[35],[36],[37],[39],[40],[42],[43],[44],[45],[46],[47],[48,49],[50],[51],[52],[53],[54],[55],[56],[57],[58],[59],[60],[61],[62],[63],[64],[65],[66],[67],[68,70,74],[69],[71],[72],[73],[75],[76],[77],[78],[79],[80]]\n",
    "\n",
    "\n",
    "predicted_clusters = []\n",
    "for cluster in cluster_results[0]['clusters']: # 2023-12-02的簇s\n",
    "    clus_index = []\n",
    "    for i in cluster['members']:\n",
    "        clus_index.append(i)\n",
    "    predicted_clusters.append(clus_index)\n",
    "print(predicted_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccee484-3279-4bf1-bd1b-940792fe0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我有一个列表如下所示[[0], [1], [2], [3], [4, 87], [5], [6, 23], [7], [8], [9], [10, 77], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21, 68], [22, 50], [24], [25], [26], [27], [28], [29, 78], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40, 43], [41, 59], [42], [44], [45], [46], [47, 88], [48], [49], [51, 67], [52], [53], [54], [55], [56], [57], [58], [60], [61], [62], [63], [64], [65], [66], [69], [70], [71], [72], [73], [74], [75], [76], [79], [80], [81], [82], [83], [84], [85], [86], [89], [90], [91], [92], [93], [94], [95]]\n",
    "# ，列表中每个子列表表示一个聚类的簇，子列表的元素个数即为簇中元素个数，然后我有一个语料文件'../Data231202-231211/Data231202.csv'，现在这个列表中的每个子列表的元素为该语料文件中的语料索引，现在我需要新增一列clus_news_num用来记录每个语料对应的簇中的元素个数，并将整个语料问价按语料对应的簇中的元素个数进行从大到小排序，要求同样大小的排名相同，然后在新增一列S_scale，该列内容为根据语料对应的簇中的元素个数进行归一化后的数，还有一列S_score为20*S_scale,还有一列index表示语料原始的坐标，最后将dataframe中的index，title,body,clus_news_num,S_scale，S_score这几列存入到新的csv文件中，请给出完整详细的代码\n",
    "# update:3.9 可跑！！\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 已有的聚类结果\n",
    "# clusters = [[0], [1], [2], [3], [4, 87], [5], [6, 23], [7], [8], [9], [10, 77], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21, 68], [22, 50], [24], [25], [26], [27], [28], [29, 78], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40, 43], [41, 59], [42], [44], [45], [46], [47, 88], [48], [49], [51, 67], [52], [53], [54], [55], [56], [57], [58], [60], [61], [62], [63], [64], [65], [66], [69], [70], [71], [72], [73], [74], [75], [76], [79], [80], [81], [82], [83], [84], [85], [86], [89], [90], [91], [92], [93], [94], [95]]\n",
    "clusters = predicted_clusters\n",
    "\n",
    "# 创建一个字典，键是语料索引，值是对应的簇大小\n",
    "index_to_cluster_size = {index: len(cluster) for cluster in clusters for index in cluster}\n",
    "\n",
    "# 读取语料文件\n",
    "df = pd.read_csv('./Data231202-231211/Data231202.csv')\n",
    "\n",
    "# 新增列clus_news_num，记录每个语料对应的簇的大小\n",
    "df['clus_news_num'] = df.index.map(index_to_cluster_size)\n",
    "\n",
    "# 根据簇大小进行排序，并添加排名，相同大小的排名相同\n",
    "df = df.sort_values(by='clus_news_num', ascending=False)\n",
    "df['rank'] = df['clus_news_num'].rank(method='min', ascending=False)\n",
    "\n",
    "# 新增列S_scale，为簇大小的归一化结果\n",
    "scaler = MinMaxScaler()\n",
    "df['S_scale'] = scaler.fit_transform(df[['clus_news_num']])\n",
    "\n",
    "# 新增列S_score，为S_scale的值乘以20\n",
    "df['S_score'] = df['S_scale'] * 20\n",
    "\n",
    "# 新增列index，表示语料原始的坐标\n",
    "df['ori_indexFrom0'] = df.index\n",
    "\n",
    "# 只保留需要的列，并保存到新的CSV文件\n",
    "final_df = df[['ori_indexFrom0', 'title', 'body', 'clus_news_num', 'rank','S_scale', 'S_score']]\n",
    "final_df.to_csv('./T1ClusterScore/final_result.csv', index=False)\n",
    "print(\"FINISH!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f628949-c3dd-497c-a79d-4bea09a1b8e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# T1-clusterScore\n",
    "# df['S_score']\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e54208-5137-418c-bcea-c0913288037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2-WebsiteScore\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83151b1e-4f00-4ad7-b1d3-5834276f444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40个网站的排名以及赋分结果在./T2WebsiteRank/website_Rank_new.csv\n",
    "# Data231202-231211/Data231202.csv\n",
    "# 读取Data231202-231211/Data231202.csv，其中的website_id为网站id，现在读取./T2WebsiteRank/website_Rank_new.csv，该文件存有website_id对应的S_task_web，现在需要将Data231202.csv中的每个语料对应的website_id对应的S_task_web新增一列进行存储，然后根据S_task_web进行排序，允许并列，新增rank列，将结果中website_id,title,S_task_web,rank存到新的csv文件\n",
    "import pandas as pd\n",
    "\n",
    "# 读取两个csv文件\n",
    "data_df = pd.read_csv('./Data231202-231211/Data231202.csv')\n",
    "rank_df = pd.read_csv('./T2WebsiteRank/website_Rank_new.csv')\n",
    "\n",
    "# 将两个DataFrame合并\n",
    "merged_df = pd.merge(data_df, rank_df, on='website_id')\n",
    "\n",
    "# 根据S_task_web列进行排序，并添加排名，相同权重的排名相同\n",
    "merged_df = merged_df.sort_values(by='S_task_web', ascending=False)\n",
    "merged_df['rank'] = merged_df['S_task_web'].rank(method='min', ascending=False)\n",
    "\n",
    "# 只保留需要的列，并保存到新的CSV文件\n",
    "final_df = merged_df[['website_id', 'title', 'S_task_web', 'rank']]\n",
    "final_df.to_csv('./T2WebsiteRank/Data231202_scoreResult.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f174e0-3182-4bdb-8a08-5bc4942108f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2-WebsiteScore\n",
    "# merged_df['S_task_web']\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026d05b-79bb-4f28-9574-5ae135404fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T3-BodyLengthScore\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb23ee-7c9b-47a0-8f33-95c31f9e4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "input_file = './Data231202-231211_FIX/Data231202_newDATA.csv'  # 替换为你的输入文件路径\n",
    "output_file = './T3BodyLenRank/Data231202_newDATA_rank_Score.csv'  # 替换为你的输出文件路径\n",
    "\n",
    "# 读取CSV文件并计算正文长度\n",
    "df = pd.read_csv(input_file)\n",
    "df['body_len'] = df['body'].apply(lambda x: len(str(x).split()))  # 假设每个单词之间用空格分隔\n",
    "\n",
    "# 按正文长度进行排序\n",
    "df = df.sort_values(by='body_len', ascending=False)\n",
    "\n",
    "# 添加排名列\n",
    "df['rank_len'] = df['body_len'].rank(method='min', ascending=False)\n",
    "\n",
    "# 计算S_scale并添加列\n",
    "max_len = df['body_len'].max()\n",
    "min_len = df['body_len'].min()\n",
    "df['S_scale'] = (df['body_len'] - min_len) / (max_len - min_len)\n",
    "\n",
    "# 计算body_len_score并添加列\n",
    "df['body_len_score'] = 20 * df['S_scale']\n",
    "\n",
    "# 保存结果到新的CSV文件\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"处理完成，并将结果保存到新的CSV文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e103b4c3-1051-4134-b5ec-299b540b1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T3-BodyLengthScore\n",
    "# df['body_len_score']\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29325066-d6c3-4574-a449-955342c3158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T4-TitleTextRankfScore\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3c0f7-ca50-426f-9b5b-c3e240e7466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 能跑！\n",
    "# 用这个！ \n",
    "# 3.9：还需检查textrank算法合理性，根据结果调整窗口大小，加上标题权重处理\n",
    "# update：加标题权重处理 标题权重为标题中每个词对应的词权重之和\n",
    "# 我现在有一个word_weight.csv文件保存的是词的权重，里面有word，weight两列分别对应词和权重，现在我需要读取‘./Data231202-231211/Data231202.csv’文件，文件中title列为新闻标题，我需要对每个title列元素进行处理，首先对其进行分词，然后找到每个词在word_weight.csv中对应的权重，将标题中所有词的权重相加得到该标题的权重，对title列的每个标题都进行处理得到每个标题的权重，然后根据标题权重进行从大到小排序，为其添加一个排名（从1开始），要求相同权重的排名相同，然后将结果存入到新的csv文件'titles_weight.csv'中，请给出详细完整的代码\n",
    "# 将标题权重归一化映射到分数！\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize # 使用NLTK进行分词，根据需要替换为适合孟加拉语的分词方法\n",
    "\n",
    "import spacy\n",
    "# from gensim.summarization import keywords\n",
    "from collections import defaultdict\n",
    "# import spacy\n",
    "import bn_core_news_sm\n",
    "from sklearn.preprocessing import MinMaxScaler # 归一化\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# import pytextrank\n",
    "# =======\n",
    "# 去除停用词\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string\n",
    "# ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c499749-4c2c-4b49-858a-17c5ef2ed0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载孟加拉语模型\n",
    "nlp = bn_core_news_sm.load()\n",
    "# # textrank算法计算权重\n",
    "# def textrank_weighted_word_graph(merged_titles):\n",
    "#     # tokens = word_tokenize(merged_titles)  # 根据实际情况替换分词方法\n",
    "#     tokens = nlp(merged_titles) # 分词\n",
    "#     print(len(tokens))\n",
    "#     print(tokens)\n",
    "    \n",
    "#     graph = nx.Graph()\n",
    "#     window_size = 80  # 根据需要调整窗口大小  # todo:调整窗口\n",
    "\n",
    "#     for i, token in enumerate(tokens):\n",
    "#         for j in range(i+1, min(i+window_size, len(tokens))):\n",
    "#             if token != tokens[j]:  # 添加边，避免自环\n",
    "#                 graph.add_edge(token, tokens[j], weight=1)\n",
    "\n",
    "#     # 使用NetworkX的PageRank算法计算每个节点（词）的权重\n",
    "#     pagerank_scores = nx.pagerank(graph, weight='weight')\n",
    "#     return pagerank_scores\n",
    "\n",
    "# update 3.9：改进版！！\n",
    "def textrank_weighted_word_graph(merged_titles):\n",
    "    tokens = nlp(merged_titles) # 分词\n",
    "    print(len(tokens))\n",
    "    print(tokens)\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "    window_size = 80  # 根据需要调整窗口大小\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        for j in range(i+1, min(i+window_size+1, len(tokens))):\n",
    "            if token != tokens[j]:  # 添加边,避免自环\n",
    "                if graph.has_edge(token, tokens[j]):\n",
    "                    graph[token][tokens[j]]['weight'] += 1 #在添加边时,先检查边是否已经存在。如果边已经存在,则将权重加1;否则,添加一个新边,权重为1。这样可以避免重复添加边。\n",
    "                else:\n",
    "                    graph.add_edge(token, tokens[j], weight=1)\n",
    "    \n",
    "    # 使用NetworkX的PageRank算法计算每个节点（词）的权重\n",
    "    pagerank_scores = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    return pagerank_scores,graph\n",
    "\n",
    "# 读取CSV文件并合并所有标题\n",
    "# df = pd.read_csv('your_file.csv')\n",
    "# df = pd.read_csv('./Data231202-231211_FIX/Data231202_newDATA.csv')\n",
    "df = pd.read_csv('./Data231202-231211/Data231202.csv')\n",
    "\n",
    "# merged_titles = ' '.join(df['title'].replace(\"\\n\", \"\"))\n",
    "merged_titles = ' '.join(title.strip() for title in df['title'])\n",
    "\n",
    "# ====================================\n",
    "# 获取孟加拉语的停用词列表\n",
    "stop_words = set(stopwords.words('bengali'))\n",
    "# print(stop_words)\n",
    "\n",
    "# 自定义标点符号列表\n",
    "custom_punctuation = ['‘', '’']\n",
    "\n",
    "# 合并 NLTK 提供的标点符号列表和自定义标点符号列表\n",
    "all_punctuation = string.punctuation + ''.join(custom_punctuation)\n",
    "\n",
    "print(all_punctuation)\n",
    "# 分词# word_tokens = word_tokenize(merged_titles)\n",
    "\n",
    "word_tokens = nlp(merged_titles) # 分词\n",
    "# word_tokens = merged_titles.split() # 根据空格分词\n",
    "token_texts = [token.text.strip() for token in word_tokens] # 去除多余空格\n",
    "\n",
    "# print(token_texts)\n",
    "print(type(token_texts))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 去除停用词\n",
    "# filtered_titles = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_titles = [w for w in token_texts if not w in stop_words] # 去除停用词\n",
    "filtered_titles = [word for word in filtered_titles if word not in all_punctuation] # 去除标点符号\n",
    "\n",
    "print(\"filtered_titles len\\n\",len(filtered_titles)) # 字符串数量！\n",
    "\n",
    "\n",
    "# 将去除停用词后的词重新组合成字符串\n",
    "# filtered_titles_text = ' '.join(filtered_titles)\n",
    "filtered_titles_text = ' '.join(filtered_titles)\n",
    "\n",
    "# print(filtered_titles_text)\n",
    "print(len(filtered_titles_text)) # 字符串长度！别被误导（所少个字符）\n",
    "# ====================================\n",
    "\n",
    "# 计算词权重\n",
    "# word_weights = textrank_weighted_word_graph(merged_titles)\n",
    "word_weights,graph = textrank_weighted_word_graph(filtered_titles_text)\n",
    "\n",
    "# 保存pagerank算法后的词关系权重 可视化\n",
    "# 根据PageRank值更新边的权重\n",
    "pagerank_weighted_graph = nx.Graph()\n",
    "for node, score in word_weights.items():\n",
    "    pagerank_weighted_graph.add_node(node)\n",
    "\n",
    "for u, v, data in graph.edges(data=True):\n",
    "    weight = data['weight'] * word_weights[u] * word_weights[v]\n",
    "    pagerank_weighted_graph.add_edge(u, v, weight=weight)\n",
    "\n",
    "with open('./T4TitleTextRank/graph_content.txt', 'w') as file:\n",
    "    file.write(str(nx.to_dict_of_dicts(pagerank_weighted_graph)))\n",
    "\n",
    "sorted_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 保存到新的CSV文件\n",
    "# word_weights_df = pd.DataFrame(word_weights.items(), columns=['word', 'weight'])\n",
    "word_weights_df = pd.DataFrame(sorted_words, columns=['word', 'weight'])\n",
    "\n",
    "\n",
    "# word_weights_df.to_csv('./T4TitleTextRank/word_weight.csv', index=False)\n",
    "word_weights_df.to_csv('./T4TitleTextRank/word_weight_new.csv', index=False)\n",
    "\n",
    "# 接下来，计算每个标题的权重\n",
    "# 读取词权重文件\n",
    "# word_weights_df = pd.read_csv('./T4TitleTextRank/word_weight.csv')\n",
    "word_weights_df = pd.read_csv('./T4TitleTextRank/word_weight_new.csv')\n",
    "\n",
    "# 将词权重转换为字典，方便查找\n",
    "word_weights = pd.Series(word_weights_df.weight.values, index=word_weights_df.word).to_dict()\n",
    "\n",
    "# 读取新闻标题文件\n",
    "titles_df = pd.read_csv('./Data231202-231211/Data231202.csv')\n",
    "# titles_df = titles_df['title']\n",
    "\n",
    "\n",
    "# 定义一个函数，用于计算标题的权重\n",
    "def calculate_title_weight(title):\n",
    "    doc = nlp(title)\n",
    "    # 对标题进行分词并计算总权重\n",
    "    # return sum(word_weights.get(token.text, 0) for token in doc)  # 如果词不在word_weights中，则默认权重为0\n",
    "    return sum(word_weights.get(token.text, 0) for token in doc if token.text not in stop_words and token.text not in string.punctuation)  # 如果词不在word_weights中，则默认权重为0\n",
    "\n",
    "\n",
    "# 计算每个标题的权重\n",
    "titles_df['weight'] = titles_df['title'].apply(calculate_title_weight)\n",
    "\n",
    "# 根据权重排序并添加排名，相同权重的排名相同\n",
    "titles_df = titles_df.sort_values(by='weight', ascending=False)\n",
    "titles_df['rank'] = titles_df['weight'].rank(method='min', ascending=False)\n",
    "\n",
    "# 对权重进行归一化处理，并存储结果到\"S_scale\"列\n",
    "scaler = MinMaxScaler()\n",
    "titles_df['S_scale'] = scaler.fit_transform(titles_df[['weight']])  # 归一化映射到分数！\n",
    "\n",
    "# 创建\"S_score\"列\n",
    "titles_df['S_score'] = titles_df['S_scale'] * 20\n",
    "\n",
    "# 只保留需要的列\n",
    "final_df = titles_df[['title', 'weight', 'rank', 'S_scale', 'S_score']]\n",
    "\n",
    "\n",
    "# 保存到新的csv文件\n",
    "# final_df.to_csv('./T4TitleTextRank/titles_weight.csv', index=False)\n",
    "final_df.to_csv('./T4TitleTextRank/titles_weight_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648c45d6-8480-4dfe-ab4b-f2e803e6a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T4-TitleTextRankfScore\n",
    "# titles_df['S_score']\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d915dab-49f9-4f9d-b9a7-03522c234dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5-CategoryScore\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae0d68c-2c21-4041-9067-8fb32212988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取新闻的category1进行类别评分\n",
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV file with category data\n",
    "category_df = pd.read_csv('./T5CateforyScore/category_score.csv')\n",
    "\n",
    "# Load the CSV file with news data\n",
    "news_df = pd.read_csv('./Data231202-231211_FIX/Data231202_newDATA.csv')\n",
    "\n",
    "# Merge the two DataFrames based on the \"category1\" column\n",
    "merged_df = pd.merge(news_df, category_df, how='left', left_on='category1', right_on='category')\n",
    "\n",
    "# Sort the merged DataFrame based on the \"rank\" column\n",
    "sorted_df = merged_df.sort_values(by='rank')\n",
    "\n",
    "# Select the desired columns\n",
    "selected_columns = ['title', 'category1', 'rank', 'S_scale', 'S_score']\n",
    "result_df = sorted_df[selected_columns]\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "result_df.to_csv('./T5CateforyScore/Data231202_categoryScore.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc782987-29cf-4638-8c78-2741b2c2b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5-CategoryScore\n",
    "# result_df['S_score']\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7759ac2-86ab-4ac4-8a2f-b2531c2d3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MergeFiveDScores\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da212b-af37-4558-9f01-0e9015df696d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-angle",
   "language": "python",
   "name": "conda-angle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
