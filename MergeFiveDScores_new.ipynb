{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9409501d-53c7-4e76-86b6-5f7a430dc264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../NewsAthm/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_26758/533468599.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_to_predict['category1'] = data_to_predict['body'].apply(predict_category)\n"
     ]
    }
   ],
   "source": [
    "# 结果还行！！！！！初版完成！！！！\n",
    "# 现在我有一个训练好的分类模型bert-base-multilingual-cased_classification_undersampled_new_epoch_20.pth，我需要加载这个分类模型对文本进行分类操作，具体描述为：有一个新闻语料文件./Data231202-231211/Data231202.csv，语料中body列为新闻文本，category1列为新闻类别，但语料中有部分语料的\n",
    "# 类别在['খেলাধুলা','রাজনীতি','বিনোদন','অর্থনীতি','আইন','শিক্ষা','বিজ্ঞান','লাইফস্টাইল','অন্যান্য']以外，我现在需要根据语料中category1类别不属于这些类别的语料进行处理，根据其新闻文本对语料的新闻类别进行预测，并将预测后的结果替换原来的类别，并将结果保存到新的csv文件中，请给出完整详细的代码\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# 加载训练好的模型\n",
    "model_path = '../NewsAthm/bert-base-multilingual-cased'\n",
    "modelNew_load_path = './classificationModel/bert-base-multilingual-cased_classification_undersampled_new_epoch_20.pth'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=9)\n",
    "\n",
    "model.load_state_dict(torch.load(modelNew_load_path))\n",
    "model.eval()\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 定义类别列表\n",
    "categories = ['খেলাধুলা', 'রাজনীতি', 'বিনোদন', 'অর্থনীতি', 'আইন', 'শিক্ষা', 'বিজ্ঞান', 'লাইফস্টাইল', 'অন্যান্য']\n",
    "\n",
    "# 读取csv文件\n",
    "data = pd.read_csv('./Data231202-231211/Data231202.csv')\n",
    "\n",
    "# 定义预测函数\n",
    "def predict_category(text):\n",
    "    # 对文本进行编码\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # 进行预测\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "    # 返回预测的类别\n",
    "    return categories[predictions.item()]\n",
    "\n",
    "# 对数据进行处理\n",
    "def process_data(data):\n",
    "    # 找出category1不在指定类别列表中的数据\n",
    "    mask = ~data['category1'].isin(categories)\n",
    "    data_to_predict = data[mask]\n",
    "\n",
    "    # 对需要预测的数据进行预测\n",
    "    data_to_predict['category1'] = data_to_predict['body'].apply(predict_category)\n",
    "\n",
    "    # 将预测后的数据与原数据合并\n",
    "    data[mask] = data_to_predict\n",
    "\n",
    "    return data\n",
    "\n",
    "# 处理数据\n",
    "processed_data = process_data(data)\n",
    "\n",
    "# 保存处理后的数据到新的csv文件\n",
    "processed_data.to_csv('./Data231202-231211/Data231202_processed.csv', index=False)\n",
    "\n",
    "print(\"FINISH!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb1d4f-7da6-4b05-b0fd-401fa8fc3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1-clusterScore\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60810f7a-f47f-4d03-a039-decf79e235bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda angle https://github.com/SeanLee97/AnglE/tree/main\n",
    "# pip install nltk\n",
    "# pip install --upgrade pip\n",
    "# pip install spacy==2.3.5\n",
    "# pip install bn_core_news_sm-0.1.0.tar.gz\n",
    "# pip install matplotlib\n",
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from angle_emb import AnglE\n",
    "\n",
    "# yes! 聚类评估！！！可跑 TP, FP, TN, FN 得到RI、Precision、Recall、F1，ARI\n",
    "# update:单个成簇的处理\n",
    "from itertools import combinations\n",
    "from math import comb\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize # 使用NLTK进行分词，根据需要替换为适合孟加拉语的分词方法\n",
    "\n",
    "import spacy\n",
    "# from gensim.summarization import keywords\n",
    "from collections import defaultdict\n",
    "import bn_core_news_sm\n",
    "from sklearn.preprocessing import MinMaxScaler # 归一化\n",
    "import matplotlib.pyplot as plt\n",
    "# import pytextrank\n",
    "# =======\n",
    "# 去除停用词\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string\n",
    "# ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eb95c30-34a2-4dc7-8d89-01766746f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ORI = pd.read_csv('./Data231202-231211/Data231202.csv') # 所有子任务都是使用这个\n",
    "data_ORI = processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfb4c0b5-6f08-41b8-96db-1f683a4cf2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用angle加载\n",
    "model_id = '../NewsAthmTask2/models/angle-bert-base-uncased-nli-en-v1'\n",
    "angle = AnglE.from_pretrained(model_id, pooling_strategy='cls_avg').cuda()\n",
    "\n",
    "# 加载数据\n",
    "data = data_ORI\n",
    "\n",
    "# 将日期转换为日期时间格式\n",
    "data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "# 获取唯一日期列表\n",
    "dates = data['pub_time'].dt.date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc5a7234-02be-4e19-a6a9-28a9af64c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义聚类中心更新函数\n",
    "def update_cluster_center(cluster):\n",
    "    cluster_embeddings = angle.encode(cluster, to_numpy=True) # 使用angle加载\n",
    "     \n",
    "    return np.mean(cluster_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19e99549-bbc8-4223-b0be-54e28bc5024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_clusters(data,threshold):\n",
    "    # 对于每个日期\n",
    "    cluster_results = []\n",
    "    cnt = 0\n",
    "    for date in dates:\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "        # 获取该日期的新闻标题\n",
    "        news_data = data[data['pub_time'].dt.date == date]['title'].tolist()\n",
    "        # 获取该日期的新闻正文\n",
    "        # news_data = data[data['pub_time'].dt.date == date]['body'].tolist() # ByBody\n",
    "\n",
    "        embeddings = angle.encode(news_data, to_numpy=True) # 使用angle加载\n",
    "\n",
    "        # 定义当天的簇列表\n",
    "        daily_clusters = []\n",
    "\n",
    "        # 对于每个新闻数据\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            # 如果簇列表为空，则新开一个簇\n",
    "            if not daily_clusters:\n",
    "                # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "                continue\n",
    "\n",
    "            # 计算当前数据点与各个簇中心的相似度\n",
    "            similarities = [cosine_similarity([embedding], [cluster['center']])[0][0] for cluster in daily_clusters]\n",
    "\n",
    "            # 找到最大相似度及其对应的簇索引\n",
    "            max_similarity = max(similarities)\n",
    "            max_index = similarities.index(max_similarity)\n",
    "\n",
    "            # 如果最大相似度大于阈值，则将当前数据点加入对应簇，并更新簇中心\n",
    "            if max_similarity > threshold:\n",
    "                daily_clusters[max_index]['members'].append(i) # 改为存index\n",
    "                daily_clusters[max_index]['news'].append(news_data[i]) # 改为存index\n",
    "                daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['news'])\n",
    "            # 否则新开一个簇\n",
    "            else:\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "\n",
    "        # 将当天的簇信息添加到结果列表中\n",
    "        cluster_results.append({'date': date, 'clusters': daily_clusters})\n",
    "\n",
    "    predicted_clusters = []\n",
    "    for cluster in cluster_results[0]['clusters']: # 2023-12-02的簇s\n",
    "        clus_index = []\n",
    "        for i in cluster['members']:\n",
    "            clus_index.append(i)\n",
    "        predicted_clusters.append(clus_index)\n",
    "    print(predicted_clusters)\n",
    "    \n",
    "    return predicted_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fccee484-3279-4bf1-bd1b-940792fe0052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[0], [1], [2], [3], [4, 87], [5], [6, 23], [7], [8], [9], [10, 77], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21, 68], [22, 50], [24], [25], [26], [27], [28], [29, 78], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40, 43], [41, 59], [42], [44], [45], [46], [47, 88], [48], [49], [51, 67], [52], [53], [54], [55], [56], [57], [58], [60], [61], [62], [63], [64], [65], [66], [69], [70], [71], [72], [73], [74], [75], [76], [79], [80], [81], [82], [83], [84], [85], [86], [89], [90], [91], [92], [93], [94], [95]]\n",
      "FINISH!\n"
     ]
    }
   ],
   "source": [
    "# 设置阈值\n",
    "threshold = 0.972\n",
    "clusters = get_predicted_clusters(data,threshold)\n",
    "\n",
    "# 创建一个字典，键是语料索引，值是对应的簇大小\n",
    "index_to_cluster_size = {index: len(cluster) for cluster in clusters for index in cluster}\n",
    "\n",
    "# 读取语料文件\n",
    "df = data_ORI\n",
    "\n",
    "# 新增列clus_news_num，记录每个语料对应的簇的大小\n",
    "df['T1_clus_news_num'] = df.index.map(index_to_cluster_size)\n",
    "\n",
    "# 根据簇大小进行排序，并添加排名，相同大小的排名相同\n",
    "df = df.sort_values(by='T1_clus_news_num', ascending=False)\n",
    "df['T1_rank'] = df['T1_clus_news_num'].rank(method='min', ascending=False)\n",
    "\n",
    "# 新增列S_scale，为簇大小的归一化结果\n",
    "scaler = MinMaxScaler()\n",
    "df['T1_S_scale'] = scaler.fit_transform(df[['T1_clus_news_num']])\n",
    "\n",
    "# 新增列S_score，为S_scale的值乘以20\n",
    "df['T1_S_score'] = df['T1_S_scale'] * 20\n",
    "\n",
    "# 新增列index，表示语料原始的坐标\n",
    "df['T1_ori_indexFrom0'] = df.index\n",
    "\n",
    "# 只保留需要的列，并保存到新的CSV文件\n",
    "T1_final_df = df[['id','T1_ori_indexFrom0', 'title', 'body', 'T1_clus_news_num', 'T1_rank','T1_S_scale', 'T1_S_score']]\n",
    "T1_final_df.to_csv('./T1ClusterScore/final_result_new.csv', index=False)\n",
    "print(\"FINISH!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f628949-c3dd-497c-a79d-4bea09a1b8e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# T1-clusterScore\n",
    "# df['S_score']\n",
    "# new: T1_final_df :T1_S_score\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e54208-5137-418c-bcea-c0913288037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2-WebsiteScore\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83151b1e-4f00-4ad7-b1d3-5834276f444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40个网站的排名以及赋分结果在./T2WebsiteRank/website_Rank_new.csv\n",
    "# Data231202-231211/Data231202.csv\n",
    "# 读取Data231202-231211/Data231202.csv，其中的website_id为网站id，现在读取./T2WebsiteRank/website_Rank_new.csv，该文件存有website_id对应的S_task_web，现在需要将Data231202.csv中的每个语料对应的website_id对应的S_task_web新增一列进行存储，然后根据S_task_web进行排序，允许并列，新增rank列，将结果中website_id,title,S_task_web,rank存到新的csv文件\n",
    "\n",
    "# 读取两个csv文件\n",
    "data_df = data_ORI\n",
    "rank_df = pd.read_csv('./T2WebsiteRank/website_Rank_new.csv')\n",
    "\n",
    "# 将两个DataFrame合并\n",
    "merged_df = pd.merge(data_df, rank_df, on='website_id')\n",
    "\n",
    "# 根据S_task_web列进行排序，并添加排名，相同权重的排名相同\n",
    "merged_df = merged_df.sort_values(by='T2_S_score', ascending=False)\n",
    "merged_df['T2_rank'] = merged_df['T2_S_score'].rank(method='min', ascending=False)\n",
    "\n",
    "# 只保留需要的列，并保存到新的CSV文件\n",
    "T2_final_df = merged_df[['id','website_id', 'title', 'T2_S_score', 'T2_rank']]\n",
    "T2_final_df.to_csv('./T2WebsiteRank/Data231202_scoreResult.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f174e0-3182-4bdb-8a08-5bc4942108f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2-WebsiteScore\n",
    "# merged_df['S_task_web']\n",
    "# T2_final_df T2_S_score\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026d05b-79bb-4f28-9574-5ae135404fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T3-BodyLengthScore\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95fb23ee-7c9b-47a0-8f33-95c31f9e4cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，并将结果保存到新的CSV文件中。\n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件并计算正文长度\n",
    "df = data_ORI\n",
    "df['body_len'] = df['body'].apply(lambda x: len(str(x).split()))  # 假设每个单词之间用空格分隔\n",
    "\n",
    "# 按正文长度进行排序\n",
    "df = df.sort_values(by='body_len', ascending=False)\n",
    "\n",
    "# 添加排名列\n",
    "df['T3_rank'] = df['body_len'].rank(method='min', ascending=False)\n",
    "\n",
    "# 计算S_scale并添加列\n",
    "max_len = df['body_len'].max()\n",
    "min_len = df['body_len'].min()\n",
    "df['T3_S_scale'] = (df['body_len'] - min_len) / (max_len - min_len)\n",
    "\n",
    "# 计算body_len_score并添加列\n",
    "df['T3_S_score'] = 20 * df['T3_S_scale']\n",
    "\n",
    "# 保存结果到新的CSV文件\n",
    "output_file = './T3BodyLenRank/Data231202_newDATA_rank_Score_new.csv'  # 替换为你的输出文件路径\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "# 只保留需要的列，并保存到新的CSV文件\n",
    "T3_final_df = df[['id','title', 'body_len', 'T3_rank','T3_S_scale', 'T3_S_score']]\n",
    "T3_final_df.to_csv('./T3BodyLenRank/Data231202_T3scoreResult.csv', index=False)\n",
    "print(\"处理完成，并将结果保存到新的CSV文件中。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e103b4c3-1051-4134-b5ec-299b540b1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T3-BodyLengthScore\n",
    "# new T3_final_df T3_S_score\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29325066-d6c3-4574-a449-955342c3158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T4-TitleTextRankfScore\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c499749-4c2c-4b49-858a-17c5ef2ed0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~‘’\n",
      "<class 'list'>\n",
      "filtered_titles len\n",
      " 607\n",
      "4350\n",
      "607\n"
     ]
    }
   ],
   "source": [
    "# 加载孟加拉语模型\n",
    "nlp = bn_core_news_sm.load()\n",
    "# # textrank算法计算权重\n",
    "# update 3.9：改进版！！\n",
    "def textrank_weighted_word_graph(merged_titles):\n",
    "    tokens = nlp(merged_titles) # 分词\n",
    "    print(len(tokens))\n",
    "    # print(tokens)\n",
    "    \n",
    "    graph = nx.Graph()\n",
    "    window_size = 80  # 根据需要调整窗口大小\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        for j in range(i+1, min(i+window_size+1, len(tokens))):\n",
    "            if token != tokens[j]:  # 添加边,避免自环\n",
    "                if graph.has_edge(token, tokens[j]):\n",
    "                    graph[token][tokens[j]]['weight'] += 1 #在添加边时,先检查边是否已经存在。如果边已经存在,则将权重加1;否则,添加一个新边,权重为1。这样可以避免重复添加边。\n",
    "                else:\n",
    "                    graph.add_edge(token, tokens[j], weight=1)\n",
    "    \n",
    "    # 使用NetworkX的PageRank算法计算每个节点（词）的权重\n",
    "    pagerank_scores = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    return pagerank_scores,graph\n",
    "\n",
    "# 读取CSV文件并合并所有标题\n",
    "df = data_ORI\n",
    "\n",
    "merged_titles = ' '.join(title.strip() for title in df['title'])\n",
    "\n",
    "# ====================================\n",
    "# 获取孟加拉语的停用词列表\n",
    "stop_words = set(stopwords.words('bengali'))\n",
    "# print(stop_words)\n",
    "\n",
    "# 自定义标点符号列表\n",
    "custom_punctuation = ['‘', '’']\n",
    "\n",
    "# 合并 NLTK 提供的标点符号列表和自定义标点符号列表\n",
    "all_punctuation = string.punctuation + ''.join(custom_punctuation)\n",
    "\n",
    "print(all_punctuation)\n",
    "# 分词# word_tokens = word_tokenize(merged_titles)\n",
    "\n",
    "word_tokens = nlp(merged_titles) # 分词\n",
    "# word_tokens = merged_titles.split() # 根据空格分词\n",
    "token_texts = [token.text.strip() for token in word_tokens] # 去除多余空格\n",
    "\n",
    "# print(token_texts)\n",
    "print(type(token_texts))\n",
    "\n",
    "\n",
    "\n",
    "# 去除停用词\n",
    "# filtered_titles = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_titles = [w for w in token_texts if not w in stop_words] # 去除停用词\n",
    "filtered_titles = [word for word in filtered_titles if word not in all_punctuation] # 去除标点符号\n",
    "\n",
    "print(\"filtered_titles len\\n\",len(filtered_titles)) # 字符串数量！\n",
    "\n",
    "# 将去除停用词后的词重新组合成字符串\n",
    "filtered_titles_text = ' '.join(filtered_titles)\n",
    "\n",
    "print(len(filtered_titles_text)) # 字符串长度！别被误导（所少个字符）\n",
    "# ====================================\n",
    "\n",
    "# 计算词权重\n",
    "word_weights,graph = textrank_weighted_word_graph(filtered_titles_text)\n",
    "\n",
    "# 保存pagerank算法后的词关系权重 可视化\n",
    "# 根据PageRank值更新边的权重\n",
    "# 记录权重关系 字典形式存储\n",
    "pagerank_weighted_graph = nx.Graph()\n",
    "for node, score in word_weights.items():\n",
    "    pagerank_weighted_graph.add_node(node)\n",
    "\n",
    "for u, v, data in graph.edges(data=True):\n",
    "    weight = data['weight'] * word_weights[u] * word_weights[v]\n",
    "    pagerank_weighted_graph.add_edge(u, v, weight=weight)\n",
    "\n",
    "with open('./T4TitleTextRank/graph_content.txt', 'w') as file:\n",
    "    file.write(str(nx.to_dict_of_dicts(pagerank_weighted_graph)))\n",
    "\n",
    "sorted_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 保存到新的CSV文件\n",
    "# word_weights_df = pd.DataFrame(word_weights.items(), columns=['word', 'weight'])\n",
    "word_weights_df = pd.DataFrame(sorted_words, columns=['word', 'weight'])\n",
    "\n",
    "\n",
    "# word_weights_df.to_csv('./T4TitleTextRank/word_weight.csv', index=False)\n",
    "word_weights_df.to_csv('./T4TitleTextRank/word_weight_new.csv', index=False)\n",
    "\n",
    "# 接下来，计算每个标题的权重\n",
    "# 读取词权重文件\n",
    "# word_weights_df = pd.read_csv('./T4TitleTextRank/word_weight.csv')\n",
    "word_weights_df = pd.read_csv('./T4TitleTextRank/word_weight_new.csv')\n",
    "\n",
    "# 将词权重转换为字典，方便查找\n",
    "word_weights = pd.Series(word_weights_df.weight.values, index=word_weights_df.word).to_dict()\n",
    "\n",
    "# print(word_weights)\n",
    "# 读取新闻标题文件\n",
    "titles_df = data_ORI\n",
    "# titles_df = pd.read_csv('./Data231202-231211/Data231202.csv')\n",
    "# titles_df = titles_df['title']\n",
    "\n",
    "\n",
    "\n",
    "# 定义一个函数，用于计算标题的权重\n",
    "def calculate_title_weight(title):\n",
    "    doc = nlp(title)\n",
    "    # 对标题进行分词并计算总权重\n",
    "    return sum(word_weights.get(token.text, 0) for token in doc)  # 如果词不在word_weights中，则默认权重为0\n",
    "    # return sum(word_weights.get(token.text, 0) for token in doc if token.text not in stop_words and token.text not in all_punctuation)  # 如果词不在word_weights中，则默认权重为0\n",
    "    # return sum(word_weights.get(token.text, 0) for token in doc if token.text not in stop_words and token.text not in string.punctuation)  # 如果词不在word_weights中，则默认权重为0\n",
    "\n",
    "\n",
    "# 计算每个标题的权重\n",
    "titles_df['T4_title_weight'] = titles_df['title'].apply(calculate_title_weight)\n",
    "# print(titles_df['T4_title_weight'])\n",
    "\n",
    "# 根据权重排序并添加排名，相同权重的排名相同\n",
    "titles_df = titles_df.sort_values(by='T4_title_weight', ascending=False)\n",
    "titles_df['T4_rank'] = titles_df['T4_title_weight'].rank(method='min', ascending=False)\n",
    "\n",
    "# 对权重进行归一化处理，并存储结果到\"S_scale\"列\n",
    "scaler = MinMaxScaler()\n",
    "titles_df['T4_S_scale'] = scaler.fit_transform(titles_df[['T4_title_weight']])  # 归一化映射到分数！\n",
    "\n",
    "# 创建\"S_score\"列\n",
    "titles_df['T4_S_score'] = titles_df['T4_S_scale'] * 20\n",
    "\n",
    "# 只保留需要的列\n",
    "T4_final_df = titles_df[['id','title', 'T4_title_weight', 'T4_rank', 'T4_S_scale', 'T4_S_score']]\n",
    "\n",
    "\n",
    "# 保存到新的csv文件\n",
    "# final_df.to_csv('./T4TitleTextRank/titles_weight.csv', index=False)\n",
    "T4_final_df.to_csv('./T4TitleTextRank/titles_weight_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648c45d6-8480-4dfe-ab4b-f2e803e6a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T4-TitleTextRankfScore\n",
    "# T4_final_df  T4_S_score\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d915dab-49f9-4f9d-b9a7-03522c234dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5-CategoryScore\n",
    "# 问题：这里的是最新的9种，而data_ORI中的类别不只9种，还需要对类别进行处理\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aae0d68c-2c21-4041-9067-8fb32212988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取新闻的category1进行类别评分\n",
    "\n",
    "category_df = pd.read_csv('./T5CateforyScore/category_score.csv')\n",
    "\n",
    "# Load the CSV file with news data\n",
    "# news_df = pd.read_csv('./Data231202-231211_FIX/Data231202_newDATA.csv')\n",
    "news_df = data_ORI\n",
    "\n",
    "\n",
    "# Merge the two DataFrames based on the \"category1\" column\n",
    "merged_df = pd.merge(news_df, category_df, how='left', left_on='category1', right_on='category')\n",
    "\n",
    "# Sort the merged DataFrame based on the \"rank\" column\n",
    "sorted_df = merged_df.sort_values(by='T5_rank')\n",
    "\n",
    "# Select the desired columns\n",
    "selected_columns = ['id','title', 'category1', 'T5_rank', 'T5_S_scale', 'T5_S_score']\n",
    "T5_final_df = sorted_df[selected_columns]\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "T5_final_df.to_csv('./T5CateforyScore/Data231202_categoryScore_new.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc782987-29cf-4638-8c78-2741b2c2b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5-CategoryScore\n",
    "# T5_final_df T5_S_score\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7759ac2-86ab-4ac4-8a2f-b2531c2d3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MergeFiveDScores\n",
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29da212b-af37-4558-9f01-0e9015df696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1_final_df :'id','T1_ori_indexFrom0', 'title', 'body', 'T1_clus_news_num', 'T1_rank','T1_S_scale', 'T1_S_score'\n",
    "# T2_final_df:'id','website_id', 'title', 'T2_S_score', 'T2_rank'\n",
    "# T3_final_df:'id','title', 'body_len', 'T3_rank','T3_S_scale', 'T3_S_score'\n",
    "# T4_final_df: 'id','title', 'T4_title_weight', 'T4_rank', 'T4_S_scale', 'T4_S_score'\n",
    "# T5_final_df:'id','title', 'category1', 'T5_rank', 'T5_S_scale', 'T5_S_score'\n",
    "# 合并5个dataframe：\n",
    "# 第一步:将T1_final_df和T2_final_df合并\n",
    "merged_df = pd.merge(T1_final_df, T2_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 第二步:将第一步合并后的DataFrame与T3_final_df合并\n",
    "merged_df = pd.merge(merged_df, T3_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 第三步:将第二步合并后的DataFrame与T4_final_df合并\n",
    "merged_df = pd.merge(merged_df, T4_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 第四步:将第三步合并后的DataFrame与T5_final_df合并\n",
    "merged_df = pd.merge(merged_df, T5_final_df, on=['id', 'title'], how='outer')\n",
    "\n",
    "# 打印合并后的 DataFrame\n",
    "merged_df.to_csv('./MergeFiveDScore/FiveDScore_Merge.csv', index=False)\n",
    "# print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67dbcb60-051f-4a98-9f7f-9a8dcfd5a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设权重 \n",
    "w1, w2, w3, w4, w5 = 0.5,0.05,0.05,0.3,0.1\n",
    "# 权重设置思路：\n",
    "# ①层次分析法 根据各任务的重要性赋权\n",
    "# ②迭代 需要一个评估指标（正确个数？）来进行迭代找出模型最优权重！\n",
    "\n",
    "# 计算总分数\n",
    "merged_df['total_S_score'] = w1 * merged_df['T1_S_score'] + w2 * merged_df['T2_S_score'] + w3 * merged_df['T3_S_score'] + w4 * merged_df['T4_S_score'] + w5 * merged_df['T5_S_score']\n",
    "\n",
    "# 生成排名\n",
    "merged_df['total_rank'] = merged_df['total_S_score'].rank(method='min', ascending=False)\n",
    "\n",
    "# 根据总分数降序排序\n",
    "merged_df = merged_df.sort_values('total_S_score', ascending=False)\n",
    "\n",
    "# 将结果保存到csv文件\n",
    "merged_df.to_csv('./MergeFiveDScore/total_result.csv', index=False)\n",
    "\n",
    "selected_columns = ['id','T1_ori_indexFrom0', 'category1','title','body','total_S_score','total_rank']\n",
    "merged_df_pure =  merged_df[selected_columns]\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "merged_df_pure.to_csv('./MergeFiveDScore/total_result_pure.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443a627-a10d-4628-b006-ba2e7cd8ae97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-angle",
   "language": "python",
   "name": "conda-angle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
