{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4594c-f17d-41fe-bbe2-e2195171bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from angle_emb import AnglE\n",
    "\n",
    "model_id = 'models/angle-bert-base-uncased-nli-en-v1'\n",
    "angle = AnglE.from_pretrained(model_id, pooling_strategy='cls_avg').cuda()\n",
    "vec = angle.encode('hello world', to_numpy=True)\n",
    "print(vec)\n",
    "vecs = angle.encode(['hello world1', 'hello world2'], to_numpy=True)\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dcc9054-3f1d-4c98-8d2f-c254859b5251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9458630021756957\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "model_id = 'models/angle-bert-base-uncased-nli-en-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id).cuda()\n",
    "\n",
    "inputs = 'hello world!'\n",
    "inputs2 = 'say hello to the world'\n",
    "tok = tokenizer([inputs], return_tensors='pt')\n",
    "tok2 = tokenizer([inputs2], return_tensors='pt')\n",
    "for k, v in tok.items():\n",
    "    tok[k] = v.cuda()\n",
    "hidden_state = model(**tok).last_hidden_state\n",
    "vec = (hidden_state[:, 0] + torch.mean(hidden_state, dim=1)) / 2.0\n",
    "# print(vec.tolist())\n",
    "for k, v in tok2.items():\n",
    "    tok2[k] = v.cuda()\n",
    "hidden_state2 = model(**tok2).last_hidden_state\n",
    "vec2 = (hidden_state[:, 0] + torch.mean(hidden_state2, dim=1)) / 2.0\n",
    "# print(vec2.tolist())\n",
    "vec_list = vec.tolist()\n",
    "vec2_list = vec2.tolist()\n",
    "cos = cosine_similarity(vec_list,vec2_list)\n",
    "print(cos[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5d771f-9134-4641-8705-b038572ed70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes! 聚类评估！！！可跑 TP, FP, TN, FN 得到RI、Precision、Recall、F1，ARI\n",
    "# update:单个成簇的处理\n",
    "from itertools import combinations\n",
    "from math import comb\n",
    "\n",
    "def evaluate_clustering(true_clusters, predicted_clusters):\n",
    "    def count_pairs(cluster):\n",
    "        pairs = list(combinations(cluster, 2))\n",
    "        return pairs\n",
    "\n",
    "    def compute_pairs(true_clusters, predicted_clusters):\n",
    "        TP, FP, TN, FN = 0, 0, 0, 0\n",
    "        base_plus = 0\n",
    "        for true_cluster in true_clusters:\n",
    "            flag_single = False\n",
    "            true_cluster = sorted(true_cluster)\n",
    "            true_pairs = count_pairs(true_cluster) # 排序处理，防止顺序造成偏差\n",
    "            if len(true_cluster) == 1:\n",
    "                true_pairs.append((true_cluster[0], true_cluster[0])) # 与自己组成元组\n",
    "                flag_single = True\n",
    "            # print(\"true_cluster:\",true_cluster)\n",
    "            # print(\"++++++++++++\")\n",
    "            # print(\"true_pairs:\",true_pairs)\n",
    "            # print(\"========\")\n",
    "            \n",
    "            for pair in true_pairs:\n",
    "                flag = False\n",
    "                for predicted_cluster in predicted_clusters:\n",
    "                    predicted_cluster = sorted(predicted_cluster) # 排序处理，防止顺序造成偏差\n",
    "                    predicted_pairs = count_pairs(predicted_cluster) \n",
    "                    if len(predicted_cluster) == 1:\n",
    "                        predicted_pairs.append((predicted_cluster[0], predicted_cluster[0])) # 与自己组成元组\n",
    "                    # print(\"predicted_cluster:\",predicted_cluster)\n",
    "                    # print(\"++++++++++++\")\n",
    "                    # print(\"predicted_pairs:\",predicted_pairs)\n",
    "                    if pair in predicted_pairs:\n",
    "                        TP += 1\n",
    "                        flag = True # true中同簇，在predict中有找到同簇\n",
    "                        if flag_single:\n",
    "                            base_plus+=1\n",
    "                if not flag: # flag为False，true中同簇，在predict中有找不到同簇\n",
    "                    FN += 1\n",
    "                    if flag_single:\n",
    "                        base_plus+=1\n",
    "        for predicted_cluster in predicted_clusters:\n",
    "            flag_single = False\n",
    "            predicted_cluster = sorted(predicted_cluster)\n",
    "            predicted_pairs = count_pairs(predicted_cluster) \n",
    "            if len(predicted_cluster) == 1:\n",
    "                predicted_pairs.append((predicted_cluster[0], predicted_cluster[0])) # 与自己组成元组\n",
    "                flag_single = True\n",
    "            for pair in predicted_pairs:\n",
    "                flag2 = False\n",
    "                for true_cluster in true_clusters:\n",
    "                    true_cluster = sorted(true_cluster)\n",
    "                    true_pairs = count_pairs(true_cluster)\n",
    "                    if len(true_cluster) == 1:\n",
    "                        true_pairs.append((true_cluster[0], true_cluster[0])) # 与自己组成元组\n",
    "                    if pair in true_pairs:\n",
    "                        flag2 = True\n",
    "                        break\n",
    "                if not flag2: # flag2为false,在predict中同簇，在true中不同簇(找不到同簇）\n",
    "                    FP += 1 \n",
    "                    if flag_single:\n",
    "                        base_plus+=1\n",
    "        len_all = 0\n",
    "        for true_cluster in true_clusters:\n",
    "            len_all += len(true_cluster)\n",
    "        print(\"len_all:\",len_all)\n",
    "        # total_pairs = TP + FP + FN\n",
    "        # TN = comb(total_pairs, 2) - TP - FP - FN\n",
    "        TN = comb(len_all, 2) - TP - FP - FN + base_plus # 加base_plus\n",
    "        # TN = comb(len_all, 2) - TP - FP - FN \n",
    "        return TP, FP, TN, FN\n",
    "\n",
    "    def compute_RI(TP, FP, TN, FN):\n",
    "        same_cluster_pairs = TP + TN\n",
    "        different_cluster_pairs = FP + FN\n",
    "\n",
    "        RI = same_cluster_pairs / (same_cluster_pairs + different_cluster_pairs)\n",
    "        return RI\n",
    "    \n",
    "    def compute_ARI(TP, FP, TN, FN):\n",
    "        UP = 2 * (TP*TN - FN*FP)\n",
    "        DOWN = (TP+FN)*(FN+TN) + (TP+FP)*(FP+TN)\n",
    "        ARI =  UP / DOWN \n",
    "        return ARI\n",
    "\n",
    "    def compute_precision_recall_f(TP, FP, FN):\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f_value = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "        return precision, recall, f_value\n",
    "\n",
    "    # Compute TP, FP, TN, FN\n",
    "    TP, FP, TN, FN = compute_pairs(true_clusters, predicted_clusters)\n",
    "    print(\"TP:\",TP)\n",
    "    print(\"FP:\",FP)\n",
    "    print(\"TN:\",TN)\n",
    "    print(\"FN:\",FN)\n",
    "\n",
    "    # Compute RI\n",
    "    RI = compute_RI(TP, FP, TN, FN)\n",
    "    # Compute ARI\n",
    "    ARI = compute_ARI(TP, FP, TN, FN)\n",
    "\n",
    "    # Compute precision, recall, F-value\n",
    "    precision, recall, f_value = compute_precision_recall_f(TP, FP, FN)\n",
    "\n",
    "    return RI, precision, recall, f_value, ARI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e6e197-35f1-4568-af1c-dc604577724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41, 59], [42], [43], [44], [45], [46], [47, 88], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [89], [90], [91], [92], [93], [94], [95]]\n",
      "len_all: 96\n",
      "TP: 69\n",
      "FP: 25\n",
      "TN: 4508\n",
      "FN: 50\n",
      "RI: 0.983877901977644\n",
      "Precision: 0.7340425531914894\n",
      "Recall: 0.5798319327731093\n",
      "F-value: 0.6478873239436621\n",
      "ARI: 0.6397536819672403\n"
     ]
    }
   ],
   "source": [
    "# 可跑版 使用angle-bert-base-uncased-nli-en-v1进行single-pass  ByTitle 使用transformer加载\n",
    "# update:加了评估，可对第一天结果进行评估与记录评估结果\n",
    "# updata:to .py :single-pass-ByTitle-angle-bert-TransformerLOAD-Eval.py\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# 加载SBERT模型\n",
    "# model_path = '/root/data/NewsAthm/sentence-transformers/distiluse-base-multilingual-cased-v2'\n",
    "# # model_path = 'distiluse-base-multilingual-cased-v2'\n",
    "# sbert_model = SentenceTransformer(model_path)\n",
    "\n",
    "model_id = 'models/angle-bert-base-uncased-nli-en-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id).cuda()\n",
    "\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('Data231202-231211.csv')\n",
    "\n",
    "# 将日期转换为日期时间格式\n",
    "data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "# 获取唯一日期列表\n",
    "dates = data['pub_time'].dt.date.unique()\n",
    "\n",
    "\n",
    "\n",
    "# 定义聚类中心更新函数\n",
    "def update_cluster_center(cluster,news_data):\n",
    "    # cluster_embeddings = sbert_model.encode(cluster)\n",
    "    # todo:\n",
    "    # 对列表中的每个新闻文本应用 tokenizer\n",
    "    cluster_embeddings = []\n",
    "    # for news in cluster:\n",
    "    for index in cluster:\n",
    "        # 使用 tokenizer 将文本转换为模型输入格式\n",
    "        # tok = tokenizer(news, return_tensors='pt')\n",
    "        tok = tokenizer(news_data[index], return_tensors='pt')\n",
    "        for k, v in tok.items():\n",
    "            tok[k] = v.cuda()\n",
    "        hidden_state = model(**tok).last_hidden_state\n",
    "        vec = (hidden_state[:, 0] + torch.mean(hidden_state, dim=1)) / 2.0\n",
    "        cluster_embeddings.append(vec.tolist())\n",
    "        \n",
    "    return np.mean(cluster_embeddings, axis=0)\n",
    "\n",
    "# 定义写入文件函数\n",
    "def write_to_file(file_path, clusters):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for cluster_info in clusters:\n",
    "            file.write(f\"News Date: {cluster_info['date']}:\\n\")\n",
    "            file.write(f\"Number of clusters: {len(cluster_info['clusters'])}\\n\")\n",
    "            for i, cluster in enumerate(cluster_info['clusters']):\n",
    "                file.write(f\"Cluster {i + 1}:\\n\")\n",
    "                file.write(f\"Number of news articles: {len(cluster['members'])}\\n\")\n",
    "                file.write(\"News articles:\\n\")\n",
    "                for index in cluster['members']:\n",
    "                    file.write(str(index) + '\\n')\n",
    "                file.write(str(cluster['news']) + '\\n')\n",
    "                file.write(\"=============\")\n",
    "\n",
    "                \n",
    "# 设置阈值\n",
    "# threshold = 0.98\n",
    "for t in range(99, 100):\n",
    "    threshold = t / 100.0\n",
    "    print(threshold)\n",
    "    # 定义簇列表\n",
    "    clusters = []\n",
    "\n",
    "    # 对于每个日期\n",
    "    cluster_results = []\n",
    "    cnt = 0\n",
    "    for date in dates:\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "        # 获取该日期的新闻标题\n",
    "        news_data = data[data['pub_time'].dt.date == date]['title'].tolist()\n",
    "\n",
    "        # 使用SBERT模型获取语义向量\n",
    "        # embeddings = sbert_model.encode(news_data)\n",
    "        # toks = \n",
    "        # print(embeddings.shape)\n",
    "        # print(embeddings)\n",
    "\n",
    "        # todo:\n",
    "        # toks = tokenizer(news_data, return_tensors='pt')\n",
    "        # 对列表中的每个新闻文本应用 tokenizer\n",
    "        data_vec = []\n",
    "        for news in news_data:\n",
    "            # 使用 tokenizer 将文本转换为模型输入格式\n",
    "            tok = tokenizer(news, return_tensors='pt')\n",
    "            for k, v in tok.items():\n",
    "                tok[k] = v.cuda()\n",
    "            hidden_state = model(**tok).last_hidden_state\n",
    "            vec = (hidden_state[:, 0] + torch.mean(hidden_state, dim=1)) / 2.0\n",
    "            data_vec.append(vec.tolist())\n",
    "\n",
    "        # 定义当天的簇列表\n",
    "        daily_clusters = []\n",
    "\n",
    "        # 对于每个新闻数据\n",
    "        for i, embedding in enumerate(data_vec):\n",
    "            # 如果簇列表为空，则新开一个簇\n",
    "            if not daily_clusters:\n",
    "                # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "                continue\n",
    "            # print(embedding)\n",
    "            # print(\"==============================================\")\n",
    "            # print(cluster['center'])\n",
    "            # print(daily_clusters)\n",
    "            # 计算当前数据点与各个簇中心的相似度\n",
    "            # similarities = [cosine_similarity([embedding], [cluster['center']])[0][0] for cluster in daily_clusters]\n",
    "            similarities = [cosine_similarity(embedding, cluster['center'])[0][0] for cluster in daily_clusters]\n",
    "            # print(similarities)\n",
    "            # print(\"==============================================\")\n",
    "            # 找到最大相似度及其对应的簇索引\n",
    "            max_similarity = max(similarities)\n",
    "            max_index = similarities.index(max_similarity)\n",
    "\n",
    "            # 如果最大相似度大于阈值，则将当前数据点加入对应簇，并更新簇中心\n",
    "            if max_similarity > threshold:\n",
    "                # daily_clusters[max_index]['members'].append(news_data[i])\n",
    "                daily_clusters[max_index]['members'].append(i) # 改为存index\n",
    "                daily_clusters[max_index]['news'].append(news_data[i]) # 改为存index\n",
    "                # daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['members'])\n",
    "                daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['members'],news_data)\n",
    "            # 否则新开一个簇\n",
    "            else:\n",
    "                # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    " \n",
    "        # 将当天的簇信息添加到结果列表中\n",
    "        cluster_results.append({'date': date, 'clusters': daily_clusters})\n",
    "\n",
    "        # 评估\n",
    "    true_clusters = [[0],[1],[2,16],[3],[4,6,22,50,73,87],[5],[7],[8,61],[9],[10,77],[11],[12],[13],\n",
    " [14,29,41,51,59,67,78,84],[15],[17],[18],[19],[20],[21,68],[23],[24],[25],[26],\n",
    " [27],[28],[30],[31],[32],[33],[34],[35,55],[36],[37],[38],[39],[40],[42],[43,64],\n",
    " [44],[45],[46],[47,53,88],[48],[49],[52],[54],[56],[57],[58],[60],[62],[63],[65],\n",
    " [66],[69],[70],[71],[72],[74],[75],[76],[79],[80],[81],[82],[83],[85],[86],\n",
    " [89],[90],[91],[92],[93],[94],[95]]\n",
    "    \n",
    "    predicted_clusters = []\n",
    "    for cluster in cluster_results[0]['clusters']: # 2023-12-02的簇s\n",
    "        clus_index = []\n",
    "        for i in cluster['members']:\n",
    "            clus_index.append(i)\n",
    "        predicted_clusters.append(clus_index)\n",
    "    print(predicted_clusters)\n",
    "        \n",
    "\n",
    "    RI, precision, recall, f_value, ARI = evaluate_clustering(true_clusters, predicted_clusters)\n",
    "    print(\"RI:\", RI)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F-value:\", f_value)\n",
    "    print(\"ARI:\", ARI)\n",
    "    # 打开文件并追加模式写入\n",
    "    \n",
    "    with open('./results/angle-bert-base-uncased-nli-en-v1-results/EVAL-single-pass-ByTitle_results.txt', 'a') as file:\n",
    "        file.write(\"threshold: \" + str(threshold) + \"\\n\")\n",
    "        file.write(\"------------------------------------\\n\")\n",
    "        file.write(\"RI: \" + str(RI) + \"\\n\")\n",
    "        file.write(\"Precision: \" + str(precision) + \"\\n\")\n",
    "        file.write(\"Recall: \" + str(recall) + \"\\n\")\n",
    "        file.write(\"F-value: \" + str(f_value) + \"\\n\")\n",
    "        file.write(\"ARI: \" + str(ARI) + \"\\n\")\n",
    "        file.write(\"====================================\\n\")\n",
    "\n",
    "    file_name = f'./results/angle-bert-base-uncased-nli-en-v1-results/index-single-pass-ByTitle_results_{threshold}.txt'\n",
    "    # 将聚类结果写入到新文件中\n",
    "    write_to_file(file_name, cluster_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24607961-d833-4068-99b6-9f2d2812339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35, 55], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47, 88], [48], [49], [50], [51], [52], [53], [54], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [89], [90], [91], [92], [93], [94], [95]]\n",
      "len_all: 96\n",
      "TP: 69\n",
      "FP: 25\n",
      "TN: 4508\n",
      "FN: 50\n",
      "RI: 0.983877901977644\n",
      "Precision: 0.7340425531914894\n",
      "Recall: 0.5798319327731093\n",
      "F-value: 0.6478873239436621\n",
      "ARI: 0.6397536819672403\n"
     ]
    }
   ],
   "source": [
    "# 可跑版 使用angle-bert-base-uncased-nli-en-v1进行single-pass  ByBody\n",
    "# update:加了评估，可对第一天结果进行评估与记录评估结果\n",
    "# update:ByBody\n",
    "# update：使用angle加载\n",
    "# updata:to .py :single-pass-ByBody-angle-bert-AngleLOAD-Eval.py\n",
    "\n",
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from angle_emb import AnglE\n",
    "\n",
    "\n",
    "\n",
    "# 加载SBERT模型\n",
    "# model_path = '/root/data/NewsAthm/sentence-transformers/distiluse-base-multilingual-cased-v2'\n",
    "# # model_path = 'distiluse-base-multilingual-cased-v2'\n",
    "# sbert_model = SentenceTransformer(model_path)\n",
    "\n",
    "# 用transformer加载\n",
    "# model_id = 'models/angle-bert-base-uncased-nli-en-v1'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModel.from_pretrained(model_id).cuda()\n",
    "\n",
    "# 使用angle加载\n",
    "model_id = 'models/angle-bert-base-uncased-nli-en-v1'\n",
    "angle = AnglE.from_pretrained(model_id, pooling_strategy='cls_avg').cuda()\n",
    "# vec = angle.encode('hello world', to_numpy=True)\n",
    "# print(vec)\n",
    "# vecs = angle.encode(['hello world1', 'hello world2'], to_numpy=True)\n",
    "# print(vecs)\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('Data231202-231211.csv')\n",
    "\n",
    "# 将日期转换为日期时间格式\n",
    "data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "# 获取唯一日期列表\n",
    "dates = data['pub_time'].dt.date.unique()\n",
    "\n",
    "\n",
    "\n",
    "# 定义聚类中心更新函数\n",
    "def update_cluster_center(cluster):\n",
    "    # cluster_embeddings = sbert_model.encode(cluster)\n",
    "    cluster_embeddings = angle.encode(cluster, to_numpy=True) # 使用angle加载\n",
    "    # todo:\n",
    "    \n",
    "    # # 对列表中的每个新闻文本应用 tokenizer\n",
    "    # cluster_embeddings = []\n",
    "    # # for news in cluster:\n",
    "    # for index in cluster:\n",
    "    #     # 使用 tokenizer 将文本转换为模型输入格式\n",
    "    #     # tok = tokenizer(news, return_tensors='pt')\n",
    "    #     tok = tokenizer(news_data[index], return_tensors='pt')\n",
    "    #     for k, v in tok.items():\n",
    "    #         tok[k] = v.cuda()\n",
    "    #     hidden_state = model(**tok).last_hidden_state\n",
    "    #     vec = (hidden_state[:, 0] + torch.mean(hidden_state, dim=1)) / 2.0\n",
    "    #     cluster_embeddings.append(vec.tolist())\n",
    "        \n",
    "    return np.mean(cluster_embeddings, axis=0)\n",
    "\n",
    "# 定义写入文件函数\n",
    "def write_to_file(file_path, clusters):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for cluster_info in clusters:\n",
    "            file.write(f\"News Date: {cluster_info['date']}:\\n\")\n",
    "            file.write(f\"Number of clusters: {len(cluster_info['clusters'])}\\n\")\n",
    "            for i, cluster in enumerate(cluster_info['clusters']):\n",
    "                file.write(f\"Cluster {i + 1}:\\n\")\n",
    "                file.write(f\"Number of news articles: {len(cluster['members'])}\\n\")\n",
    "                file.write(\"News articles:\\n\")\n",
    "                for index in cluster['members']:\n",
    "                    file.write(str(index) + '\\n')\n",
    "                file.write(str(cluster['news']) + '\\n')\n",
    "                file.write(\"=============\")\n",
    "\n",
    "                \n",
    "# 设置阈值\n",
    "# threshold = 0.98\n",
    "for t in range(90, 100):\n",
    "    threshold = t / 100.0\n",
    "    print(threshold)\n",
    "    # 定义簇列表\n",
    "    clusters = []\n",
    "\n",
    "    # 对于每个日期\n",
    "    cluster_results = []\n",
    "    cnt = 0\n",
    "    for date in dates:\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "        # 获取该日期的新闻标题\n",
    "        # news_data = data[data['pub_time'].dt.date == date]['title'].tolist()\n",
    "        # 获取该日期的新闻正文\n",
    "        news_data = data[data['pub_time'].dt.date == date]['body'].tolist() # ByBody\n",
    "\n",
    "        # 使用SBERT模型获取语义向量\n",
    "        # embeddings = sbert_model.encode(news_data)\n",
    "        \n",
    "        embeddings = angle.encode(news_data, to_numpy=True) # 使用angle加载\n",
    "        \n",
    "        # toks = \n",
    "        # print(embeddings.shape)\n",
    "        # print(embeddings)\n",
    "\n",
    "        # # todo:\n",
    "        # # toks = tokenizer(news_data, return_tensors='pt')\n",
    "        # # 对列表中的每个新闻文本应用 tokenizer\n",
    "        # data_vec = []\n",
    "        # for news in news_data:\n",
    "        #     # 使用 tokenizer 将文本转换为模型输入格式\n",
    "        #     tok = tokenizer(news, return_tensors='pt')\n",
    "        #     for k, v in tok.items():\n",
    "        #         tok[k] = v.cuda()\n",
    "        #     hidden_state = model(**tok).last_hidden_state\n",
    "        #     vec = (hidden_state[:, 0] + torch.mean(hidden_state, dim=1)) / 2.0\n",
    "        #     data_vec.append(vec.tolist())\n",
    "\n",
    "        # 定义当天的簇列表\n",
    "        daily_clusters = []\n",
    "\n",
    "        # 对于每个新闻数据\n",
    "        # for i, embedding in enumerate(data_vec):\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            # 如果簇列表为空，则新开一个簇\n",
    "            if not daily_clusters:\n",
    "                # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "                continue\n",
    "            # print(embedding)\n",
    "            # print(\"==============================================\")\n",
    "            # print(cluster['center'])\n",
    "            # print(daily_clusters)\n",
    "            # 计算当前数据点与各个簇中心的相似度\n",
    "            # similarities = [cosine_similarity([embedding], [cluster['center']])[0][0] for cluster in daily_clusters]\n",
    "            similarities = [cosine_similarity([embedding], [cluster['center']])[0][0] for cluster in daily_clusters]\n",
    "            # print(similarities)\n",
    "            # print(\"==============================================\")\n",
    "            # 找到最大相似度及其对应的簇索引\n",
    "            max_similarity = max(similarities)\n",
    "            max_index = similarities.index(max_similarity)\n",
    "\n",
    "            # 如果最大相似度大于阈值，则将当前数据点加入对应簇，并更新簇中心\n",
    "            if max_similarity > threshold:\n",
    "                # daily_clusters[max_index]['members'].append(news_data[i])\n",
    "                daily_clusters[max_index]['members'].append(i) # 改为存index\n",
    "                daily_clusters[max_index]['news'].append(news_data[i]) # 改为存index\n",
    "                # daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['members'])\n",
    "                # daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['members'],news_data)\n",
    "                daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['news'])\n",
    "            # 否则新开一个簇\n",
    "            else:\n",
    "                # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    " \n",
    "        # 将当天的簇信息添加到结果列表中\n",
    "        cluster_results.append({'date': date, 'clusters': daily_clusters})\n",
    "\n",
    "        # 评估\n",
    "    true_clusters = [[0],[1],[2,16],[3],[4,6,22,50,73,87],[5],[7],[8,61],[9],[10,77],[11],[12],[13],\n",
    " [14,29,41,51,59,67,78,84],[15],[17],[18],[19],[20],[21,68],[23],[24],[25],[26],\n",
    " [27],[28],[30],[31],[32],[33],[34],[35,55],[36],[37],[38],[39],[40],[42],[43,64],\n",
    " [44],[45],[46],[47,53,88],[48],[49],[52],[54],[56],[57],[58],[60],[62],[63],[65],\n",
    " [66],[69],[70],[71],[72],[74],[75],[76],[79],[80],[81],[82],[83],[85],[86],\n",
    " [89],[90],[91],[92],[93],[94],[95]]\n",
    "    \n",
    "    predicted_clusters = []\n",
    "    for cluster in cluster_results[0]['clusters']: # 2023-12-02的簇s\n",
    "        clus_index = []\n",
    "        for i in cluster['members']:\n",
    "            clus_index.append(i)\n",
    "        predicted_clusters.append(clus_index)\n",
    "    print(predicted_clusters)\n",
    "        \n",
    "\n",
    "    RI, precision, recall, f_value, ARI = evaluate_clustering(true_clusters, predicted_clusters)\n",
    "    print(\"RI:\", RI)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F-value:\", f_value)\n",
    "    print(\"ARI:\", ARI)\n",
    "    # 打开文件并追加模式写入\n",
    "    \n",
    "    # with open('./results/angle-bert-base-uncased-nli-en-v1-results/EVAL-single-pass-ByTitle_results.txt', 'a') as file:\n",
    "    with open('./results/angle-bert-base-uncased-nli-en-v1-results/EVAL-single-pass-ByBody_results.txt', 'a') as file: # ByBody\n",
    "        file.write(\"threshold: \" + str(threshold) + \"\\n\")\n",
    "        file.write(\"LOAD BY:\" + \"ANGLE\" + \"\\n\")\n",
    "        file.write(\"------------------------------------\\n\")\n",
    "        file.write(\"RI: \" + str(RI) + \"\\n\")\n",
    "        file.write(\"Precision: \" + str(precision) + \"\\n\")\n",
    "        file.write(\"Recall: \" + str(recall) + \"\\n\")\n",
    "        file.write(\"F-value: \" + str(f_value) + \"\\n\")\n",
    "        file.write(\"ARI: \" + str(ARI) + \"\\n\")\n",
    "        file.write(\"====================================\\n\")\n",
    "\n",
    "    # file_name = f'./results/angle-bert-base-uncased-nli-en-v1-results/index-single-pass-ByTitle_results_{threshold}.txt'\n",
    "    file_name = f'./results/angle-bert-base-uncased-nli-en-v1-results/index-single-pass-ByBody-ANGLE-LOAD_results_{threshold}.txt' # ByBody\n",
    "\n",
    "    # 将聚类结果写入到新文件中\n",
    "    write_to_file(file_name, cluster_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4800eec1-8146-4f3a-be04-997ef9ac3935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[[0], [1], [2], [3], [4, 87], [5], [6, 23], [7], [8], [9], [10, 77], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [24], [25], [26], [27], [28], [29, 78], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40, 43], [41, 59], [42], [44], [45], [46], [47, 88], [48], [49], [50], [51, 67], [52], [53], [54], [55], [56], [57], [58], [60], [61], [62], [63], [64], [65], [66], [68], [69], [70], [71], [72], [73], [74], [75], [76], [79], [80], [81], [82], [83], [84], [85], [86], [89], [90], [91], [92], [93], [94], [95]]\n",
      "len_all: 96\n",
      "TP: 71\n",
      "FP: 17\n",
      "TN: 4506\n",
      "FN: 48\n",
      "RI: 0.9859974149073675\n",
      "Precision: 0.8068181818181818\n",
      "Recall: 0.5966386554621849\n",
      "F-value: 0.6859903381642511\n",
      "ARI: 0.6789935634874196\n"
     ]
    }
   ],
   "source": [
    "# 可跑版 使用angle-bert-base-uncased-nli-en-v1进行single-pass  ByTitle\n",
    "# update:加了评估，可对第一天结果进行评估与记录评估结果\n",
    "# updata:ByTitle\n",
    "# update：使用angle加载\n",
    "# updata:to .py :single-pass-ByTitle-angle-bert-AngleLOAD-Eval.py\n",
    "\n",
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from angle_emb import AnglE\n",
    "\n",
    "\n",
    "\n",
    "# 加载SBERT模型\n",
    "# model_path = '/root/data/NewsAthm/sentence-transformers/distiluse-base-multilingual-cased-v2'\n",
    "# # model_path = 'distiluse-base-multilingual-cased-v2'\n",
    "# sbert_model = SentenceTransformer(model_path)\n",
    "\n",
    "# 用transformer加载\n",
    "# model_id = 'models/angle-bert-base-uncased-nli-en-v1'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModel.from_pretrained(model_id).cuda()\n",
    "\n",
    "# 使用angle加载\n",
    "model_id = 'models/angle-bert-base-uncased-nli-en-v1'\n",
    "angle = AnglE.from_pretrained(model_id, pooling_strategy='cls_avg').cuda()\n",
    "# vec = angle.encode('hello world', to_numpy=True)\n",
    "# print(vec)\n",
    "# vecs = angle.encode(['hello world1', 'hello world2'], to_numpy=True)\n",
    "# print(vecs)\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('Data231202-231211.csv')\n",
    "\n",
    "# 将日期转换为日期时间格式\n",
    "data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "# 获取唯一日期列表\n",
    "dates = data['pub_time'].dt.date.unique()\n",
    "\n",
    "\n",
    "\n",
    "# 定义聚类中心更新函数\n",
    "def update_cluster_center(cluster):\n",
    "    # cluster_embeddings = sbert_model.encode(cluster)\n",
    "    cluster_embeddings = angle.encode(cluster, to_numpy=True) # 使用angle加载\n",
    "    # todo:\n",
    "    \n",
    "    # # 对列表中的每个新闻文本应用 tokenizer\n",
    "    # cluster_embeddings = []\n",
    "    # # for news in cluster:\n",
    "    # for index in cluster:\n",
    "    #     # 使用 tokenizer 将文本转换为模型输入格式\n",
    "    #     # tok = tokenizer(news, return_tensors='pt')\n",
    "    #     tok = tokenizer(news_data[index], return_tensors='pt')\n",
    "    #     for k, v in tok.items():\n",
    "    #         tok[k] = v.cuda()\n",
    "    #     hidden_state = model(**tok).last_hidden_state\n",
    "    #     vec = (hidden_state[:, 0] + torch.mean(hidden_state, dim=1)) / 2.0\n",
    "    #     cluster_embeddings.append(vec.tolist())\n",
    "        \n",
    "    return np.mean(cluster_embeddings, axis=0)\n",
    "\n",
    "# 定义写入文件函数\n",
    "def write_to_file(file_path, clusters):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for cluster_info in clusters:\n",
    "            file.write(f\"News Date: {cluster_info['date']}:\\n\")\n",
    "            file.write(f\"Number of clusters: {len(cluster_info['clusters'])}\\n\")\n",
    "            for i, cluster in enumerate(cluster_info['clusters']):\n",
    "                file.write(f\"Cluster {i + 1}:\\n\")\n",
    "                file.write(f\"Number of news articles: {len(cluster['members'])}\\n\")\n",
    "                file.write(\"News articles:\\n\")\n",
    "                for index in cluster['members']:\n",
    "                    file.write(str(index) + '\\n')\n",
    "                file.write(str(cluster['news']) + '\\n')\n",
    "                file.write(\"=============\")\n",
    "\n",
    "                \n",
    "# 设置阈值\n",
    "# threshold = 0.98\n",
    "for t in range(90, 100):\n",
    "    threshold = t / 100.0\n",
    "    print(threshold)\n",
    "    # 定义簇列表\n",
    "    clusters = []\n",
    "\n",
    "    # 对于每个日期\n",
    "    cluster_results = []\n",
    "    cnt = 0\n",
    "    for date in dates:\n",
    "        print(cnt)\n",
    "        cnt+=1\n",
    "        # 获取该日期的新闻标题\n",
    "        news_data = data[data['pub_time'].dt.date == date]['title'].tolist()\n",
    "        # 获取该日期的新闻正文\n",
    "        # news_data = data[data['pub_time'].dt.date == date]['body'].tolist() # ByBody\n",
    "\n",
    "        # 使用SBERT模型获取语义向量\n",
    "        # embeddings = sbert_model.encode(news_data)\n",
    "        \n",
    "        embeddings = angle.encode(news_data, to_numpy=True) # 使用angle加载\n",
    "        \n",
    "        # toks = \n",
    "        # print(embeddings.shape)\n",
    "        # print(embeddings)\n",
    "\n",
    "        # # todo:\n",
    "        # # toks = tokenizer(news_data, return_tensors='pt')\n",
    "        # # 对列表中的每个新闻文本应用 tokenizer\n",
    "        # data_vec = []\n",
    "        # for news in news_data:\n",
    "        #     # 使用 tokenizer 将文本转换为模型输入格式\n",
    "        #     tok = tokenizer(news, return_tensors='pt')\n",
    "        #     for k, v in tok.items():\n",
    "        #         tok[k] = v.cuda()\n",
    "        #     hidden_state = model(**tok).last_hidden_state\n",
    "        #     vec = (hidden_state[:, 0] + torch.mean(hidden_state, dim=1)) / 2.0\n",
    "        #     data_vec.append(vec.tolist())\n",
    "\n",
    "        # 定义当天的簇列表\n",
    "        daily_clusters = []\n",
    "\n",
    "        # 对于每个新闻数据\n",
    "        # for i, embedding in enumerate(data_vec):\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            # 如果簇列表为空，则新开一个簇\n",
    "            if not daily_clusters:\n",
    "                # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    "                continue\n",
    "            # print(embedding)\n",
    "            # print(\"==============================================\")\n",
    "            # print(cluster['center'])\n",
    "            # print(daily_clusters)\n",
    "            # 计算当前数据点与各个簇中心的相似度\n",
    "            # similarities = [cosine_similarity([embedding], [cluster['center']])[0][0] for cluster in daily_clusters]\n",
    "            similarities = [cosine_similarity([embedding], [cluster['center']])[0][0] for cluster in daily_clusters]\n",
    "            # print(similarities)\n",
    "            # print(\"==============================================\")\n",
    "            # 找到最大相似度及其对应的簇索引\n",
    "            max_similarity = max(similarities)\n",
    "            max_index = similarities.index(max_similarity)\n",
    "\n",
    "            # 如果最大相似度大于阈值，则将当前数据点加入对应簇，并更新簇中心\n",
    "            if max_similarity > threshold:\n",
    "                # daily_clusters[max_index]['members'].append(news_data[i])\n",
    "                daily_clusters[max_index]['members'].append(i) # 改为存index\n",
    "                daily_clusters[max_index]['news'].append(news_data[i]) # 改为存index\n",
    "                # daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['members'])\n",
    "                # daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['members'],news_data)\n",
    "                daily_clusters[max_index]['center'] = update_cluster_center(daily_clusters[max_index]['news'])\n",
    "            # 否则新开一个簇\n",
    "            else:\n",
    "                # daily_clusters.append({'center': embedding, 'members': [news_data[i]]})\n",
    "                daily_clusters.append({'center': embedding, 'members': [i],'news':[news_data[i]]}) # 改为存index\n",
    " \n",
    "        # 将当天的簇信息添加到结果列表中\n",
    "        cluster_results.append({'date': date, 'clusters': daily_clusters})\n",
    "\n",
    "        # 评估\n",
    "    true_clusters = [[0],[1],[2,16],[3],[4,6,22,50,73,87],[5],[7],[8,61],[9],[10,77],[11],[12],[13],\n",
    " [14,29,41,51,59,67,78,84],[15],[17],[18],[19],[20],[21,68],[23],[24],[25],[26],\n",
    " [27],[28],[30],[31],[32],[33],[34],[35,55],[36],[37],[38],[39],[40],[42],[43,64],\n",
    " [44],[45],[46],[47,53,88],[48],[49],[52],[54],[56],[57],[58],[60],[62],[63],[65],\n",
    " [66],[69],[70],[71],[72],[74],[75],[76],[79],[80],[81],[82],[83],[85],[86],\n",
    " [89],[90],[91],[92],[93],[94],[95]]\n",
    "    \n",
    "    predicted_clusters = []\n",
    "    for cluster in cluster_results[0]['clusters']: # 2023-12-02的簇s\n",
    "        clus_index = []\n",
    "        for i in cluster['members']:\n",
    "            clus_index.append(i)\n",
    "        predicted_clusters.append(clus_index)\n",
    "    print(predicted_clusters)\n",
    "        \n",
    "\n",
    "    RI, precision, recall, f_value, ARI = evaluate_clustering(true_clusters, predicted_clusters)\n",
    "    print(\"RI:\", RI)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F-value:\", f_value)\n",
    "    print(\"ARI:\", ARI)\n",
    "    # 打开文件并追加模式写入\n",
    "    \n",
    "    with open('./results/angle-bert-base-uncased-nli-en-v1-results/EVAL-single-pass-ByTitle-ANGLE-LOAD_results.txt', 'a') as file:\n",
    "    # with open('./results/angle-bert-base-uncased-nli-en-v1-results/EVAL-single-pass-ByBody_results.txt', 'a') as file: # ByBody\n",
    "        file.write(\"threshold: \" + str(threshold) + \"\\n\")\n",
    "        file.write(\"LOAD BY:\" + \"ANGLE\" + \"\\n\")\n",
    "        file.write(\"------------------------------------\\n\")\n",
    "        file.write(\"RI: \" + str(RI) + \"\\n\")\n",
    "        file.write(\"Precision: \" + str(precision) + \"\\n\")\n",
    "        file.write(\"Recall: \" + str(recall) + \"\\n\")\n",
    "        file.write(\"F-value: \" + str(f_value) + \"\\n\")\n",
    "        file.write(\"ARI: \" + str(ARI) + \"\\n\")\n",
    "        file.write(\"====================================\\n\")\n",
    "\n",
    "    file_name = f'./results/angle-bert-base-uncased-nli-en-v1-results/index-single-pass-ByTitle-ANGLE-LOAD_results_{threshold}.txt'\n",
    "    # file_name = f'./results/angle-bert-base-uncased-nli-en-v1-results/index-single-pass-ByBody-ANGLE-LOAD_results_{threshold}.txt' # ByBody\n",
    "\n",
    "    # 将聚类结果写入到新文件中\n",
    "    write_to_file(file_name, cluster_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02b92497-a47f-4a24-afe0-076c7ed8b34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一维度的长度： 96\n",
      "第二维度的长度： 768\n",
      "第一维度的长度： 119\n",
      "第二维度的长度： 768\n",
      "第一维度的长度： 81\n",
      "第二维度的长度： 768\n",
      "第一维度的长度： 75\n",
      "第二维度的长度： 768\n",
      "第一维度的长度： 129\n",
      "第二维度的长度： 768\n",
      "第一维度的长度： 136\n",
      "第二维度的长度： 768\n",
      "第一维度的长度： 99\n",
      "第二维度的长度： 768\n",
      "第一维度的长度： 122\n",
      "第二维度的长度： 768\n",
      "第一维度的长度： 129\n",
      "第二维度的长度： 768\n",
      "第一维度的长度： 34\n",
      "第二维度的长度： 768\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "model_id = 'models/angle-bert-base-uncased-nli-en-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id).cuda()\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('Data231202-231211.csv')\n",
    "\n",
    "# 将日期转换为日期时间格式\n",
    "data['pub_time'] = pd.to_datetime(data['pub_time'])\n",
    "\n",
    "# 获取唯一日期列表\n",
    "dates = data['pub_time'].dt.date.unique()\n",
    "\n",
    "# 设置阈值\n",
    "threshold = 0.95\n",
    "\n",
    "# 定义簇列表\n",
    "clusters = []\n",
    "\n",
    "# 对于每个日期\n",
    "cluster_results = []\n",
    "for date in dates:\n",
    "    # 获取该日期的新闻标题\n",
    "    news_data = data[data['pub_time'].dt.date == date]['title'].tolist()\n",
    "    \n",
    "    # 使用SBERT模型获取语义向量\n",
    "    embeddings = sbert_model.encode(news_data)\n",
    "    \n",
    "    # toks = tokenizer(news_data, return_tensors='pt')\n",
    "    # 对列表中的每个新闻文本应用 tokenizer\n",
    "    data_vec = []\n",
    "    for news in news_data:\n",
    "        # 使用 tokenizer 将文本转换为模型输入格式\n",
    "        tok = tokenizer(news, return_tensors='pt')\n",
    "        for k, v in tok.items():\n",
    "            tok[k] = v.cuda()\n",
    "        hidden_state = model(**tok).last_hidden_state\n",
    "        vec = (hidden_state[:, 0] + torch.mean(hidden_state, dim=1)) / 2.0\n",
    "        data_vec.append(vec.tolist())\n",
    "\n",
    "\n",
    "    # # 输出处理后的结果\n",
    "    # print(tokenized_data)\n",
    "    \n",
    "    # print(data_vec)\n",
    "    # print(data_vec.size())\n",
    "    # 获取列表的长度（第一维度的长度）\n",
    "    list_length = len(data_vec)\n",
    "    print(\"第一维度的长度：\", list_length)\n",
    "\n",
    "    # 获取第二维度的长度（假设所有子列表的长度相同）\n",
    "    if list_length > 0:\n",
    "        sub_list_length = len(data_vec[0][0])\n",
    "        print(\"第二维度的长度：\", sub_list_length)\n",
    "    \n",
    "# inputs = 'hello world!'\n",
    "# inputs2 = 'say hello to the world'\n",
    "# tok = tokenizer([inputs], return_tensors='pt')\n",
    "# tok2 = tokenizer([inputs2], return_tensors='pt')\n",
    "# for k, v in tok.items():\n",
    "#     tok[k] = v.cuda()\n",
    "# hidden_state = model(**tok).last_hidden_state\n",
    "# vec = (hidden_state[:, 0] + torch.mean(hidden_state, dim=1)) / 2.0\n",
    "# # print(vec.tolist())\n",
    "# for k, v in tok2.items():\n",
    "#     tok2[k] = v.cuda()\n",
    "# hidden_state2 = model(**tok2).last_hidden_state\n",
    "# vec2 = (hidden_state[:, 0] + torch.mean(hidden_state2, dim=1)) / 2.0\n",
    "# # print(vec2.tolist())\n",
    "# vec_list = vec.tolist()\n",
    "# vec2_list = vec2.tolist()\n",
    "# cos = cosine_similarity(vec_list,vec2_list)\n",
    "# print(cos[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6e64d-5e3f-4aff-b5ec-4364b1aab07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122fad77-8f0e-4d32-b95d-1016950dfcb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-default",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
